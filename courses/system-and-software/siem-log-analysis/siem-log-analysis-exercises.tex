\documentclass[a4paper,11pt,notitlepage]{report}
% Henrik Kramselund, February 2001
% hlk@security6.net,
% My standard packages
\usepackage{kea-exercises}

\begin{document}

%\rm
\selectlanguage{english}

\selectlanguage{english}
\newcommand{\subject}[1]{SIEM and Log Analysis course}
\mytitle{SIEM and Log Analysis}{exercises}

\setcounter{tocdepth}{0}



{\color{titlecolor}
\renewcommand{\baselinestretch}{0.3}\setlength{\parskip}{1mm}
\tableofcontents}
%\listoffigures - not used
%\listoftables - not used

\normal
\pagestyle{fancyplain}
\chapter*{\color{titlecolor}Preface}
\markboth{Preface}{}
\setlength{\parskip}{3mm}

This material is prepared for use in \emph{\subject} and was prepared by
Henrik Kramselund Jereminsen, \link{http://www.zencurity.com} .
It describes the networking setup and
applications for trainings and courses where hands-on exercises are needed.

Further a presentation is used which is available as PDF from kramse@Github\\
Look for \jobname in the repo security-courses.

These exercises are expected to be performed in a training setting with network connected systems. The exercises use a number of tools which can be copied and reused after training. A lot is described about setting up your workstation in the repo

\link{https://github.com/kramse/kramse-labs}


\section*{\color{titlecolor}Prerequisites}

This material expect that participants have a working knowledge of
TCP/IP from a user perspective. Basic concepts such as web site addresses and email should be known as well as IP-addresses and common protocols like DHCP.

\vskip 1cm
Have fun and learn
\eject

% =================== body of the document ===============
% Arabic page numbers
\rhead{\fancyplain{}{\bf \chaptername\ \thechapter}}

% Main chapters
%---------------------------------------------------------------------
% gennemgang af emnet
% check questions

\chapter*{\color{titlecolor}Exercise content}
\markboth{Exercise content}{}

Most exercises follow the same procedure and has the following content:
\begin{itemize}
\item {\bf Objective:} What is the exercise about, the objective
\item {\bf Purpose:} What is to be the expected outcome and goal of doing this exercise
\item {\bf Suggested method:} suggest a way to get started
\item {\bf Hints:} one or more hints and tips or even description how to
do the actual exercises
\item {\bf Solution:} one possible solution is specified
\item {\bf Discussion:} Further things to note about the exercises, things to remember and discuss
\end{itemize}

Please note that the method and contents are similar to real life scenarios and does not detail every step of doing the exercises. Entering commands directly from a book only teaches typing, while the exercises are designed to help you become able to learn and actually research solutions.


\chapter{\faExclamationTriangle\ Download Debian Administrator’s Handbook -- 10 min}
\label{ex:sw-downloadDEB}

\hlkimage{3cm}{book-debian-administrators-handbook.jpg}


{\bf Objective:}\\
We need a Linux for running some tools during the course. I have chosen Debian Linux as this is open source, and the developers have released a whole book about running it.

This book is named
\emph{The Debian Administrator’s Handbook},  - shortened DEB

{\bf Purpose:}\\
We need to install Debian Linux in a few moments, so better have the instructions ready.

{\bf Suggested method:}\\
Create folders for educational materials. Go to download from the link \url{https://debian-handbook.info/}
Read and follow the instructions for downloading the book.

{\bf Solution:}\\
When you have a directory structure for download for this course, and the book DEB in PDF you are done.

{\bf Discussion:}\\
Linux is free and everywhere. The tools we will run in this course are made for Unix, so they run great on Linux.

Debian Linux is a free operating system platform.

The book DEB is free, but you can buy/donate to Debian, and I recommend it.

Not curriculum but explains how to use Debian Linux


\chapter{\faExclamationTriangle\ Check your Debian VM -- 10min}
\label{ex:sw-basicDebianVM}

\hlkimage{7cm}{debian-xfce.png}

{\bf Objective:}\\
Make sure your virtual machine is in working order.

We need a Debian Linux for running tools during the course.

{\bf Purpose:}\\
If your VM is not installed and updated we will run into trouble later.

{\bf Suggested method:}\\
Go to \link{https://github.com/kramse/kramse-labs/}

Read the instructions for the setup of a Debian VM.


{\bf Hints:}\\
If you allocate enough memory and disk you wont have problems.

{\bf I suggest 50G disk, 2CPU cores and 6Gb memory for this course, if you have this.}

{\bf Solution:}\\
When you have a updated virtualisation software and a running VM, then we are good.

{\bf Discussion:}\\
Linux is free and everywhere. The tools we will run in this course are made for Unix, so they run great on Linux.

Debian Linux allows us to run Ansible and provision a whole SIEM in very few minutes.


\chapter{\faInfoCircle\ Investigate /etc -- 10min}
\label{ex:sw-basicLinuxetc}


{\bf Objective:}\\
We will investigate the /etc directory on Linux. We need a Debian Linux

{\bf Purpose:}\\
Start seeing example configuration files, including:
\begin{itemize}
  \item User database \verb+/etc/passwd+ and \verb+/etc/group+
  \item The password database \verb+/etc/shadow+
\end{itemize}

{\bf Suggested method:}\\
Boot your Linux VMs, log in

Investigate permissions for the user database files \verb+passwd+ and \verb+shadow+

{\bf Hints:}\\
Linux has many tools for viewing files, the most efficient would be less.

\begin{minted}[fontsize=\footnotesize]{shell}
user@debian:~$ cd /etc
user@debian:/etc$ ls -l shadow passwd
-rw-r--r-- 1 root root   2203 Mar 26 17:27 passwd
-rw-r----- 1 root shadow 1250 Mar 26 17:27 shadow
user@debian:/etc$ ls
... all files and directories shown, investigate more if you like
\end{minted}

Showing a single file: \verb+less /etc/passwd+ and press q to quit

Showing multiple files: \verb+less /etc/*+ then :n for next and q for quit

Trying reading the shadow file as your regular user:
\begin{minted}[fontsize=\footnotesize]{shell}
user@debian:/etc$ cat /etc/shadow
cat: /etc/shadow: Permission denied
\end{minted}

Why is that? Try switching to root, using su or sudo, and redo the command.

{\bf Solution:}\\
When you have seen the files listed you are done.

Also note the difference between running as root and normal user. Usually books and instructions will use a prompt of hash mark \verb+#+ when the root user is assumed and dollar sign \verb+$+ when a normal user prompt.

{\bf Discussion:}\\
Linux is free and everywhere. The tools we will run in this course are made for Unix, so they run great on Linux.

Sudo is a tool often used for allowing users to perform certain tasks as the super user. The tool is named from superuser do! \link{https://en.wikipedia.org/wiki/Sudo}


\chapter{\faExclamationTriangle\ Enable UFW firewall -- 10min}
\label{ex:debian-firewall}

{\bf Objective:}\\
Turn on a firewall and configure a few simple rules.

{\bf Purpose:}\\
See how easy it is to restrict incoming connections to a server.


{\bf Suggested method:}\\
Install a utility for firewall configuration.

You could also perform Nmap port scan with the firewall enabled and disabled.

{\bf Hints:}\\
Using the ufw package it is very easy to configure the firewall on Linux.

Install and configuration can be done using these commands.
\begin{minted}[fontsize=\footnotesize]{shell}
root@debian:~# apt install ufw
Reading package lists... Done
Building dependency tree
Reading state information... Done
The following NEW packages will be installed:
  ufw
0 upgraded, 1 newly installed, 0 to remove and 0 not upgraded.
Need to get 164 kB of archives.
After this operation, 848 kB of additional disk space will be used.
Get:1 http://mirrors.dotsrc.org/debian stretch/main amd64 ufw all 0.35-4 [164 kB]
Fetched 164 kB in 2s (60.2 kB/s)
...
root@debian:~# ufw allow 22/tcp
Rules updated
Rules updated (v6)
root@debian:~# ufw enable
Command may disrupt existing ssh connections. Proceed with operation (y|n)? y
Firewall is active and enabled on system startup
root@debian:~# ufw status numbered
Status: active

     To                         Action      From
     --                         ------      ----
[ 1] 22/tcp                     ALLOW IN    Anywhere
[ 2] 22/tcp (v6)                ALLOW IN    Anywhere (v6)
\end{minted}

Also allow port 80/tcp and port 443/tcp - and install a web server. Recommend Nginx \verb+apt-get install nginx+

{\bf Solution:}\\
When firewall is enabled and you can still connect to Secure Shell (SSH) and web service, you are done.

{\bf Discussion:}\\
Further configuration would often require adding source prefixes which are allowed to connect to specific services. If this was a database server the database service should probably not be reachable from all of the Internet.

Web interfaces also exist, but are more suited for a centralized firewall.

Configuration of this firewall can be done using ansible, see the documentation and examples at \url{https://docs.ansible.com/ansible/latest/modules/ufw_module.html}

Should you have both a centralized firewall in front of servers, and local firewall on each server? Discuss within your team.




\chapter{\faExclamationTriangle\ Postman API Client -- 20min}
\label{ex:postman-api}

\hlkimage{13cm}{postman-debian.png}

{\bf Objective:}\\
Get a program capable of sending REST HTTP calls installed.

{\bf Purpose:}\\
Debugging REST is often needed, and some tools like Elasticsearch is both configured and maintained using REST APIs.

{\bf Suggested method:}\\
Download the app from \link{https://www.postman.com/downloads/}

Note: the file may be named a specific version, or may include the name \emph{latest} YMMV, replace the real name when unpacking below!

On your Debian, after downloading the binary from the web site:
\begin{minted}[fontsize=\footnotesize]{shell}
user@debian:~$ mkdir bin
user@debian:~$ cd bin
user@debian:~/bin$ tar zxf ../Downloads/postman-linux-x64.tar.gz // insert real name in this command
user@debian:~/bin$ ./Postman/Postman
\end{minted}

{\bf Hints:}\\
You can run the application without signing in anywhere.

{\bf Solution:}\\
When you have performed a REST call from within this tool, you are done.

Example: use the fake site \link{https://jsonplaceholder.typicode.com/todos/1} and other similar methods from the same (fake) REST API

If you have Elasticsearch installed and running try: \link{https://127.0.0.1:9200}
Note: this is using a self-signed certificate, and also requires user login in version 8.

{\bf Discussion:}\\
Multiple applications and plugins can perform similar functions. This is a standalone app.


\chapter{\faExclamationTriangle\ Git tutorials -- 15min}
\label{ex:git-tutorial}


\hlkimage{3cm}{git-logo.png}

{\bf Objective:}\\
Try the program Git locally on your workstation

{\bf Purpose:}\\
Running Git will allow you to clone repositories from others easily. This is a great way to get new software packages, and share your own.

Git is the name of the tool, and Github is a popular site for hosting git repositories.

{\bf Suggested method:}\\
Run the program from your Linux VM. You can also clone from your Windows or Mac OS X computer. Multiple graphical front-end programs exist too.

First make sure your system is updated, as root run:

\begin{minted}[fontsize=\footnotesize]{shell}
sudo apt-get update && apt-get -y upgrade && apt-get -y dist-upgrade
\end{minted}
You should reboot if the kernel is upgraded :-)

Second make sure your system has Git, ansible and my playbooks: (as root run, or with sudo as shown)
\begin{minted}[fontsize=\footnotesize]{shell}
sudo apt -y install ansible git
\end{minted}

Most important are Git clone and pull:
\begin{minted}[fontsize=\footnotesize]{shell}
user@debian:~$ git clone https://github.com/kramse/kramse-labs.git
Cloning into 'kramse-labs'...
remote: Enumerating objects: 283, done.
remote: Total 283 (delta 0), reused 0 (delta 0), pack-reused 283
Receiving objects: 100% (283/283), 215.04 KiB | 898.00 KiB/s, done.
Resolving deltas: 100% (145/145), done.

user@debian:~$ cd kramse-labs/

user@debian:~/kramse-labs$ ls
LICENSE  README.md  core-net-lab  lab-network  suricatazeek  work-station
user@debian:~/kramse-labs$ git pull
Already up to date.
\end{minted}

{\bf Hints:}\\
Browse the Git tutorials on \link{https://git-scm.com/docs/gittutorial}\\
and \link{https://guides.github.com/activities/hello-world/}

We will not do the whole tutorials within 15 minutes, but get an idea of the command line, and see examples. Refer back to these tutorials when needed or do them at home.

Note: you dont need an account on Github to download/clone repositories, but having an acccount allows you to save repositories yourself and is recommended.

{\bf Solution:}\\
When you have tried the tool and seen the tutorials you are done.

{\bf Discussion:}\\
Before Git there has been a range of version control systems,\\
see \link{https://en.wikipedia.org/wiki/Version\_control} for more details.




\chapter{\faExclamationTriangle\ Getting started with the Elastic Stack -- 60min}
\label{gettingstartedelastic}

\hlkimage{8cm}{illustrated-screenshot-hero-kibana.png}


{\bf Objective:}\\
Get a working Elasticsearch, so we can do requests.

{\bf Purpose:}\\
We need some tools to demonstrate SIEM systems. Elasticsearch is a search engine and document store used in a lot of different systems, allowing cross application integration.

Elasticsearch uses REST extensively in their application.

{\bf Suggested method:}\\
Visit the web page for \emph{Getting started with the Elastic Stack} :\\
{\footnotesize\url{https://www.elastic.co/guide/en/elastic-stack-get-started/current/get-started-elastic-stack.html}}

Read about the tools, and the steps needed for manual installation.

When installing I highly recommend my Ansible based approach - which allows installation in less than 10 minutes automatically. It can also later be re-used in your own organizations to create a proof-of-concept or even production systems.

The ansible is described in exercise \ref{ex:basicansible} on page \pageref{ex:basicansible}

{\bf Hints:}\\
Elasticsearch can store almost anything we like.

The web page for the getting started show multiple sections:
\begin{itemize}
\item Elasticsearch - the core engine, this must be done manually or with Ansible
\item Kibana - the analytics and visualization platform
\item Beats - data shippers, a way to get some data into ES
\item Logstash (optional) offers a large selection of plugins to help you parse, enrich, transform, and buffer data from a variety of sources
\end{itemize}

Each describes a part and are recommended reading.


{\bf Solution:}\\
When you have browsed the page you are done.

{\bf Discussion:}\\
We could have used a lot of other servers and service, which ones would you prefer?



\chapter{\faExclamationTriangle\ Use Ansible to install programs -- 10-60min}
\label{ex:basicansible}


{\bf Objective:}\\
Run Elasticsearch

{\bf Purpose:}\\
See an example tool used for many projects, Elasticsearch from the Elastic Stack

{\bf Suggested method:}\\
We will run Elasticsearch, either using the method from:\\{\footnotesize
\url{https://www.elastic.co/guide/en/elastic-stack-get-started/current/get-started-elastic-stack.html}}

or by the method described below using Ansible - your choice.

{\bf I have tested my Ansible configuration with Debian 12 Bookworm and Elasticsearch version 8.10.} The settings for this server are:
\begin{itemize}
\item 50Gb disk -- about 15Gb used
\item 8Gb memory -- 1Gb free shortly after rebooting first time
\item 4 virtual CPUs -- for speed, less can be used
\end{itemize}

Ansible used below is a configuration management tool \url{https://www.ansible.com/} and you can adjust them for production use!

I try to test my playbooks using both Ubuntu and Debian Linux, but Debian is the main target for this training.

First make sure your system is updated, as root run:

\begin{minted}[fontsize=\footnotesize]{shell}
apt-get update && apt-get -y upgrade && apt-get -y dist-upgrade
\end{minted}

You should reboot if the kernel is upgraded :-)

Second make sure your system has ansible and my playbooks: (as root run)
\begin{minted}[fontsize=\footnotesize]{shell}
apt -y install ansible git python
git clone https://github.com/kramse/kramse-labs
\end{minted}

We will run the playbooks locally, while a normal Ansible setup would use SSH to connect to the remote node.

Then it should be easy to run Ansible playbooks, like this: (again as root, most packet sniffing things will also need to run as root later)

\begin{minted}[fontsize=\footnotesize]{shell}
cd kramse-labs/suricatazeek
ansible-playbook -v 1-dependencies.yml 2-suricatazeek.yml 3-elasticstack.yml 4-configuration.yml
\end{minted}

Note: I keep these playbooks flat and simple, but you should investigate Ansible roles for real deployments.

If I update these, it might be necessary to update your copy of the playbooks. Run this while you are in the cloned repository:

\begin{minted}[fontsize=\footnotesize]{shell}
git pull
\end{minted}

Note: usually I would recommend running git clone as your personal user, and then use sudo command to run some commands as root. In a training environment it is OK if you want to run everything as root. Just beware.

Note: these instructions are originally from the course\\
Go to \url{https://github.com/kramse/kramse-labs/tree/master/suricatazeek}

{\bf Hints:}\\
Ansible is great for automating stuff, so by running the playbooks we can get a whole lot of programs installed, files modified - avoiding the Vi editor \smiley

Example playbook content, installing software using APT:
\begin{minted}[fontsize=\footnotesize]{yaml}
apt:
    name: "{{ packages }}"
    vars:
      packages:
        - nmap
        - curl
        - iperf
        ...
\end{minted}

{\bf Solution:}\\
When you have a updated VM and Ansible running, then we are good.

{\bf Discussion:}\\
Linux is free and everywhere. The tools we will run in this course are made for Unix, so they run great on Linux.

When installing applications it is recommended to install the repository definition, as that will allow you to update more easily later by using \verb+apt update && apt upgrade+

\chapter{\faExclamationTriangle\ Configure Elasticsearch passwords -- 15 min}
\label{ex:elastic-password}

{\bf Objective:}\\
Make sure we can use and connect to Elasticsearch after installing it.

{\bf Purpose:}\\
Elasticsearch is being configured with more security settings on by default. One such setting are the usernames and passwords for built-in users.

{\bf Suggested method:}\\
Run the programs below from your Linux VM after installing Elasticsearch with Ansible.

When starting Elasticsearch after installing the Apt packages using Ansible you can find in the logs, \verb+/var/log/elasticsearch/elasticsearch.log+ :
\begin{alltt}
[//timestamp//][INFO ][o.e.x.s.InitialNodeSecurityAutoConfiguration] [debian-lab-12] Auto-configuration will not generate a password for the elastic built-in superuser, as we cannot  determine if there is a terminal attached to the elasticsearch process. You can use the `bin/elasticsearch-reset-password` tool to set the password for the elastic user.
\end{alltt}

This tool is located in a directory which is probably not in your PATH, so execute it directly:\\
\verb+/usr/share/elasticsearch/bin/elasticsearch-reset-password+ like this, interactive and resetting the password for the elastic user.

\begin{alltt}
root@debian-lab-12:/var/log/elasticsearch# /usr/share/elasticsearch/bin/elasticsearch-reset-password -i -u elastic
This tool will reset the password of the [elastic] user.
You will be prompted to enter the password.
Please confirm that you would like to continue [y/N]y


Enter password for [elastic]:
Re-enter password for [elastic]:
Password for the [elastic] user successfully reset.
\end{alltt}

{\bf Hints:}\\
You can confirm the password change using a browser for \url{https://127.0.0.1:9200}


{\bf Solution:}\\
When you have used the tool and seen Elasticsearch working in the browser you are done.

{\bf Discussion:}\\
I think we can all agree that logging data is security critical. So the strategy of configuring security settings by default should be applauded. Even if it makes bootstrapping harder, it ensure that the resulting setup is more \emph{production ready}.

\chapter{\faExclamationTriangle\ Configure Kibana -- 15 min}
\label{ex:kibana-enrollment}

{\bf Objective:}\\
We also need to connect Kibana -- the web application.

{\bf Purpose:}\\
Elasticsearch and Kibana is being configured with more security settings on by default. One such setting are the enrollment process for Kibana.

{\bf Suggested method:}\\
Run the programs below from your Linux VM after installing Elasticsearch with Ansible.

Check that Kibana is running, using a browser visit \url{http://127.0.0.1:5601}\\
Note: this is NOT using HTTPS \smiley

When starting Kibana after installing the application asks for an enrollment token. This can be created using a tool \verb+elasticsearch-create-enrollment-token+.

This tool is located in a directory which is probably not in your PATH, so execute it directly:\\
\verb+/usr/share/elasticsearch/bin/elasticsearch-create-enrollment-token+ like this:


\begin{alltt}
root@debian-lab-12:/root# /usr/share/elasticsearch/bin/elasticsearch-create-enrollment-token -s kibana
eyJ2ZXIiOiI4LjE...0Q5USJ9 // token of several lines
\end{alltt}

Then after pasting this into Kibana, it needs a verification code,

\begin{alltt}
root@debian-lab-12:~# /usr/share/kibana/bin/kibana-verification-code
Your verification code is:  835 291 // your code will be different
\end{alltt}

After this the service is ready for our adventures into SIEM using Elasticsearch as an example.

{\bf Hints:}\\
You can confirm the password change using a browser for \url{https://127.0.0.1:5601}


{\bf Solution:}\\
When you have used the tool and seen Kibana working in the browser you are done.

{\bf Discussion:}\\
{\LARGE Make sure to create a backup when everything is working. Most virtualization software allow you to do a backup of the virtual disk, or create a \emph{snapshot}}


\chapter{\faInfoCircle\ Install JupyterLab -- up to 30min}
\label{ex-python-Jupyterlab}

\hlkimage{17cm}{python-jupyter.png}

{\bf Objective:}\\
Try using a programing library in the Python and R environment JupyterLab.

{\bf Purpose:}\\
See a way to run the examples from the book in a nice environment.

{\bf Suggested method:}\\
Make sure Python3 PIP and R language are installed, as root do:
\begin{minted}[fontsize=\footnotesize]{shell}
root@debian:~# apt install python3-pip r-base
\end{minted}

Install jupyterlab using \verb+pip3+:
\begin{minted}[fontsize=\footnotesize]{shell}
root@debian:~# pip3 install jupyterlab
# ... lots of output
\end{minted}

\eject

Install jupyterlab \emph{kernel} using \verb+R+:
\begin{minted}[fontsize=\footnotesize]{shell}
root@debian:~# R                     // note this is a command named R, single capital

R version 4.0.4 (2021-02-15) -- "Lost Library Book"
Copyright (C) 2021 The R Foundation for Statistical Computing
Platform: x86_64-pc-linux-gnu (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

  Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> install.packages('IRkernel')
# ... lots of output
> IRkernel::installspec(user = FALSE)
[InstallKernelSpec] Installed kernelspec ir in /usr/local/share/jupyter/kernels/ir
> q()
\end{minted}



{\bf Hints:}\\
You can also just run JupyterLab on the web \smiley

{\bf Solution:}\\
When you can start JupyterLab and run Python3 from a Markdown document, you are done.


{\bf Discussion:}\\
Jupyter is a whole ecosystem and there is a lot of documentation available.

The main reason for installing it in this course is to make R and Python available in a more user-friendly manner.



\chapter{\faInfoCircle\ Making requests to Elasticsearch -- 15-75min}
\label{ex:es-rest-api}

%\hlkimage{10cm}{kali-linux.png}

{\bf Objective:}\\
Use APIs for accessing Elasticsearch data, both internal and user data.

{\bf Purpose:}\\
Learn how to make requests to an API.

{\bf Suggested method:}\\
Go to the list of exposed Elasticsearch REST APIs:\\
\link{https://www.elastic.co/guide/en/elasticsearch/reference/current/rest-apis.html}

The Elasticsearch REST APIs are exposed using JSON over HTTP.

Select a category example, Cluster APIs, then select Nodes Info APIs. This will show URLs you can use:

Warning: since Elasticsearch version 8 the main protocol used is HTTPS, and requires user login. The cURL program can work with username and password, but you need to add them:

\begin{minted}[fontsize=\footnotesize]{shell}
hlk@debian-lab-12:~$ curl http:/127.0.0.1:9200
curl: (52) Empty reply from server
# fails because it should use https

hlk@debian-lab-12:~$ curl https:/127.0.0.1:9200
curl: (60) SSL certificate problem: self-signed certificate in certificate chain
More details here: https://curl.se/docs/sslcerts.html

curl failed to verify the legitimacy of the server and therefore could not
establish a secure connection to it. To learn more about this situation and
how to fix it, please visit the web page mentioned above.
# fails due to self-signed certificate, add -k to curl command

hlk@debian-lab-12:~$ curl -k https:/127.0.0.1:9200
{"error":{"root_cause":[{"type":"security_exception","reason":"missing authentication credentials
 for REST request [/]","header":{"WWW-Authenticate":["Basic realm=\"security\"
 charset=\"UTF-8\"","Bearer realm=\"security\"","ApiKey"]}}],"type":"security_exception",
 "reason":"missing authentication credentials for REST request [/]",
 "header":{"WWW-Authenticate":["Basic realm=\"security\" charset=\"UTF-8\"",
 "Bearer realm=\"security\"", "ApiKey"]}},"status":401}
 / fails due to missing username and password

hlk@debian-lab-12:~$ curl -k https:/127.0.0.1:9200 --basic -u elastic
Enter host password for user 'elastic':
{
  "name" : "debian-lab-12",
  "cluster_name" : "elasticsearch",
  "cluster_uuid" : "2xQz_VUJS2eJCvDCuI_3ow",
  "version" : {
    "number" : "8.10.4",
    "build_flavor" : "default",
    "build_type" : "deb",
    "build_hash" : "b4a62ac808e886ff032700c391f45f1408b2538c",
    "build_date" : "2023-10-11T22:04:35.506990650Z",
    "build_snapshot" : false,
    "lucene_version" : "9.7.0",
    "minimum_wire_compatibility_version" : "7.17.0",
    "minimum_index_compatibility_version" : "7.0.0"
  },
  "tagline" : "You Know, for Search"
}
# works, and you can add it after username with :
# curl -k https:/127.0.0.1:9200 --basic -u elastic:henrik42

\end{minted}

\begin{minted}[fontsize=\footnotesize]{shell}
# return just process
curl -X GET "localhost:9200/_nodes/process?pretty"
# same as above
curl -X GET "localhost:9200/_nodes/_all/process?pretty"

curl -X GET "localhost:9200/_nodes/plugins?pretty"

# return just jvm and process of only nodeId1 and nodeId2
curl -X GET "localhost:9200/_nodes/nodeId1,nodeId2/jvm,process?pretty"
# same as above
curl -X GET "localhost:9200/_nodes/nodeId1,nodeId2/info/jvm,process?pretty"
# return all the information of only nodeId1 and nodeId2
curl -X GET "localhost:9200/_nodes/nodeId1,nodeId2/_all?pretty"
\end{minted}


{\bf Hints:}\\
Pretty Results can be obtained using the pretty parameter.
\begin{quote}
When appending ?pretty=true to any request made, the JSON returned will be pretty formatted (use it for debugging only!). Another option is to set ?format=yaml which will cause the result to be returned in the (sometimes) more readable yaml format.
\end{quote}

Lots of tutorials exist for accessing Elasticsearch, also Postman is a popular tool for making REST requests.

A couple of examples:
\begin{itemize}
\item \link{https://aws.amazon.com/blogs/database/elasticsearch-tutorial-a-quick-start-guide/}
\item \link{https://www.digitalocean.com/community/tutorials/how-to-install-elasticsearch-logstash-and-kibana-elastic-stack-on-ubuntu-18-04}
\end{itemize}

{\bf Solution:}\\
When you have seen examples of the API, understand the references with underscore, like \verb+_nodes+ and pretty printing you are done.

I recommend playing with Elasticsearch plugins and X-pack.\\
\link{https://www.elastic.co/downloads/x-pack}

Note: In versions 6.3 and later, X-Pack is included with the default distributions of Elastic Stack, with all free features enabled by default.

Also Kibana can be used for creating nice dashboards and become applications more or less.

{\bf Discussion:}\\
You can also try calling the REST API from Python

Similar to this:
\inputminted{python}{programs/rest-1.py}




% 1-overview starts here


\chapter{\faExclamationTriangle\ Mitre ATT\&CK Framework 10 min}
\label{ex:mitre-attack}


\hlkimage{14cm}{mitre-attack.png}

Source:  Great resource for attack categorization



{\bf Objective:}\\
See examples of attack methods used by real actors.


{\bf Purpose:}\\
When analyzing incidents we often need to understand how they gained acccess, moved inside the network, what they did to escalate privileges and finally exfiltrate data.

{\bf Suggested method:}\\
Go to the web site \url{https://attack.mitre.org/}, browse the matrix and read a bit here and there.

Browse the ATT\&CK 101 Blog Post\\
\url{https://medium.com/mitre-attack/att-ck-101-17074d3bc62}


{\bf Hints:}\\
The columns can be thought of as a progression. An attacker might perform recon first, then gain initial access etc. all the way to the right most columns.

{\bf Solution:}\\
When you have researched a few details in the model you are done.

{\bf Discussion:}\\
This is a large model which evolved over many years. You are not expected to remember it all, or understand it all.



\chapter{\faInfoCircle\ Brim desktop app 20 min}
\label{ex:brim-security}

\hlkimage {7cm}{brim-nitroba.png}

{\bf Objective:}\\
Try running Zeek through a desktop app. Zeek is an advanced open source network security monitoring tool that can decode network packets, either live or using packet capture files. \url{https://zeek.org/}

{\bf Purpose:}\\
You might be presented with a packet capture file, that must be analyzed. Brim is packaged as a desktop app, built with Electron just like many other applications. Once installed, you can open a pcap with Brim and it will transform the pcap into Zeek logs in the ZNG format.

{\bf Suggested method:}\\
Use either your normal operating system or the Debian VM. Then download the Brim desktop application from:
\url{https://www.brimdata.io/download/}

Download a sample packet capture:\\
\url{http://downloads.digitalcorpora.org/corpora/scenarios/2008-nitroba/nitroba.pcap}


{\bf Hints:}\\
Download the .deb file for your Debian and install using:
\verb+$ sudo dpkg -i brim_amd64.deb+

Then open a packet capture, nitroba.pcap is a common example used.

{\bf Solution:}\\
When you have browsed the Brim web site you are done, better if you managed to run it.

{\bf Discussion:}\\
We often need a combination of tools, like Wireshark with GUI and Tcpdump with command line.

Here we have Brim with GUI and Zeek for command line and production use.

Use the tool you like best for the task at hand.



\chapter{\faInfoCircle\ Run small programs: Python, Shell script 20min}
\label{ex:small-python}

{\bf Objective:}\\
Be able to create small scripts using Python and Unix shell.

{\bf Purpose:}\\
Often it is needed to automate some task. Using scripting languages allows one to quickly automate.

Python is a very popular programming language. The Python language
is an interpreted, high-level, general-purpose programming language. Created by Guido van Rossum and first released in 1991.


You can read more about Python at:\\
\url{https://www.python.org/about/gettingstarted/} and \\
\url{https://en.wikipedia.org/wiki/Python_(programming_language)}

Shell scripting is another method for automating things on Unix. There are a number of built-in shell programs available.

You should aim at using basic shell scripts, to be used with \verb+/bin/sh+ - as this is the most portable Bourne shell.



{\bf Suggested method:}\\
Both shell and Python is often part of Linux installations.

Use and editor, leafpad, pulsar, VI/VIM, joe, EMACS, Nano ...

Create two files, I named them \verb+python-example.py+ and \verb+shell-example.sh+:

\VerbatimInput{python-example.py}

\VerbatimInput{shell-example.sh}

Unix does not require the file type .py or .sh, but it is often recommended to use it. To be able to run these programs you need to make them executable. Use the commands to set execute bit and run them:



Note: Python is available in two versions, version 2 and version 3. You should aim at running only version 3, as the older one is deprecated.

{\bf Hints:}\\
\begin{alltt}
$ chmod +x python-example.py shell-example.sh

$ ./python-example.py
21

$ ./shell-example.sh
Todays date in ISO format is: 2019-08-29
This system has 32 /etc/passwd users

\end{alltt}

{\bf Solution:}\\
When you have tried making both a shell script and a python program, you are done.

{\bf Discussion:}\\
If you want to learn better shell scripting there is an older but very recommended book,

\emph{Classic Shell Scripting
Hidden Commands that Unlock the Power of Unix}
By Arnold Robbins, Nelson Beebe. Publisher: O'Reilly Media
Release Date: December 2008
 \url{http://shop.oreilly.com/product/9780596005955.do}

You can also decide to always use PowerShell for your scripting needs, your decision.




\chapter{\faInfoCircle\ Use a XML library in Python -- up to 30min}
\label{ex-python-library}

{\bf Objective:}\\
Try using a programing library in the Python programming language.

{\bf Purpose:}\\
See how easy it is to produce functionality by re-using existing functions and features available in a popular language.

{\bf Suggested method:}\\
Start by getting an XML file. Suggested method is to boot your Linux and run a command like \verb+nmap -p 80,443 -A -oA testfile www.zencurity.com+. Output should be \verb+testfile.xml+ and two other files, grepable output \verb+testfile.gnmap+ and text output \verb+testfile.nmap+.

Then using Python import a library to parse XML and print a few values from the XML, or all of them.

Recommended values to print from the file:
\begin{list2}
\item Nmap version
\item Date of the Nmap run, note either use start and convert from Unix time or startstr which is a string
\item Nmaprun args - aka the command line
\item Host address
\item Ports like from the <port protocol="tcp" portid="443">
\item Anything you feel like
\end{list2}

{\bf Hints:}\\
One option is to use the Python ElementTree XML API:\\
\url{https://docs.python.org/3/library/xml.etree.elementtree.html}

Also - use Python3!

{\bf Solution:}\\
When you can read a file and process it using Python3.

Improvements, you might consider:
\begin{list2}
\item Use Python3 to run the Nmap process
\item Create command line parameters for the program, making it more useful
\item Pretty print using formatted output
\end{list2}
{\bf Discussion:}\\
Many examples contain code like this:

\begin{quote}
Getting child tags attribute value in a XML using ElementTree

Parse the XML file and get the root tag and then using [0] will give us first child tag. Similarly [1], [2] gives us subsequent child tags. After getting child tag use \verb+.attrib[attribute_name]+ to get value of that attribute.
\end{quote}

\begin{minted}[fontsize=\footnotesize]{python}
>>> import xml.etree.ElementTree as ET
>>> xmlstr = '<foo><bar key="value">text</bar></foo>'
>>> root = ET.fromstring(xmlstr)
>>> root.tag
'foo'
>>> root[0].tag
'bar'
>>> root[0].attrib['key']
'value'
\end{minted}
Source:\\{\footnotesize \url{https://stackoverflow.com/questions/4573237/how-to-extract-xml-attribute-using-python-elementtree}}

What is the point of referring to a specific numbered child, when we specifically have the tags?!

\chapter{\faInfoCircle\ Clone a Python library StevenBlack/hosts -- 30min}
\label{ex:git-clone-XX}

{\bf Objective:}\\
Find an interesting library on Github, suggest the one called:

\link{https://github.com/StevenBlack/hosts}

{\bf Purpose:}\\
Being able to find libraries on Github can make your life easier.

We will use an example that can download \verb+hosts+ files, what is a hosts file?
{\bf Suggested method:}\\
Clone the repository in your Linux VM

Browse the readme.md

{\bf Hints:}\\
Python package manager \verb+pip3+ makes it easy to install dependencies for a program.

Install the dependencies with:

\begin{minted}[fontsize=\footnotesize]{shell}
pip3 install --user -r requirements.txt
\end{minted}


{\bf Solution:}\\
When you have read about the tool you are done.

We don't need to run the program today, but feel free to do so.

{\bf Discussion:}\\
Are these sources recommendable?

Make sure you read the license, read about the program, possibly inspect the source code.

AND data downloaded also often have restrictions


\chapter{\faExclamationTriangle\ Date Formats -- 15min}
\label{ex:dateformats}

%\hlkimage{10cm}{kali-linux.png}

{\bf Objective:}\\
See an example of time parsing, and realize how difficult time can be.

{\bf Purpose:}\\
System integration often works with different representations of the same data. Time and dates are one aspect we often meet. Realize how complex it is.

{\bf Suggested method:}\\
Visit the web pages of an existing tool, Logstash we will use througout the course and a standard for time and dates.

{\bf Write down todays date on a piece of paper, each one does their own.}

Then lookup ISO 8601\\
\url{https://en.wikipedia.org/wiki/ISO_8601}

I recommend looking at a specific system, used for processing computer logs:
Logstash \\
\url{https://www.elastic.co/guide/en/logstash/current/plugins-filters-date.html}


{\bf Hints:}\\
When you receive a date there are so many formats, that you need to be very specific how to interpret it.

Parsing dates is a complex task, best left for existing frameworks and functions.

If you decide to parse dates using your own code, then centralize it - so you can update it when you find bugs.

{\bf Solution:}\\
When you have a logstash reading a date, or via a Grok debugger on the web

{\bf Discussion:}\\
Make sure to visit the web page:\\
\url{https://infiniteundo.com/post/25326999628/falsehoods-programmers-believe-about-time}

Did you realize how complex time and computers are?

Then consider this software bug:\\
"No, you're not crazy. Open Office can't print on Tuesdays."\\
\url{https://bugs.launchpad.net/ubuntu/+source/file/+bug/248619}

Linked from\\
{\footnotesize\url{https://www.reddit.com/r/linux/comments/9hdam/no_youre_not_crazy_open_office_cant_print_on/}}

Because a command \verb+file+ has an error in parsing data, files with PostScript data - print jobs with the text Tue - are interpreted as being Erlang files instead. This breaks the printing, on Tue(sdays).

We will go through this bug in detail together.


\chapter{\faInfoCircle\ Grok Debugger -- 30min}
\label{ex:grokdebugger1}

\hlkimage{10cm}{grok-debugger.png}

{\bf Objective:}\\
Try parsing dates using an existing system.

{\bf Purpose:}\\
See how existing systems can support advanced parsing, without programming.

{\bf Suggested method:}\\
Go to the web application Grok Debugger:\\
\url{https://grokdebug.herokuapp.com/}

Try entering data into the input field, and a parsing expression in the Pattern field.

Try the data from
\url{https://www.elastic.co/guide/en/kibana/current/xpack-grokdebugger.html}



{\bf Hints:}\\
The expression with greedy data is nice for matching a lot of text:\\
\verb+%{GREEDYDATA:message}+

Try adding some text at the end of the input, and another part of the parsing with this.

{\bf Solution:}\\
When you have parsed a line and seen it you are done.

{\bf Discussion:}\\
The functionality Grok debugging is included in the tool Kibana from Elastic:\\
\url{https://www.elastic.co/guide/en/kibana/current/xpack-grokdebugger.html}




% 2-hello-world starts here


\chapter{\faExclamationTriangle\ IP address research 30 min}
\label{ex:ip-address-research}

{\bf Objective:}\\
Work with IP addresses

{\bf Purpose:}\\
What is an IP address?

Investigate the following IP addresses
\begin{list2}
\item 192.168.1.1
\item 192.0.2.0/24
\item 172.25.0.1
\item 182.129.62.63
\item 185.129.62.63
\end{list2}

Write down everything you can about them!

{\bf Suggested method:}\\
Search for the addresses, look for web sites that may help.

{\bf Hints:}\\
Download the fun guide from Julia Evans (b0rk) \url{https://jvns.ca/networking-zine.pdf}

Pay attention to Notation Time page

Lookup \url{ripe.net} they may have a service called stats or stat -- something like that.

What is the Torproject? good, bad, neutral?

{\bf Solution:}\\
When you have found some information about each of the above, say 2-3 facts about each you are done.

{\bf Discussion:}\\
IP addresses are much more than an integer used for addressing system interfaces and routing packets.

We will later talk more about \emph{IP reputation}


\chapter{\faExclamationTriangle\ Data types: IP addresses -- 15min}
\label{ex:data-types-ip-address}

{\bf Objective:}\\
Find out what IP-addresses really are -- just a 32-bit integer for IPv4.


{\bf Purpose:}\\
When working with IP addresses, it can be more efficient to use math, than string matching!


{\bf Suggested method:}\\
Lets visit the DDS book, page 73 and onwards. Quoted here for ease of use:


\begin{quote}
Since you know an 8-bit byte can range in value from 0 to 255, you also know the dotted-decimal range is 0.0.0.0 through 255.255.255.255, which is 32 bits. If you count the possible address space, you have a total of 4,294,967,296 possible addresses (the maximum value of a 32-bit integer). This brings up another point of storing and handling IP addresses: {\bf Any IP address can be converted to/from a 32-bit integer value.}

This is important because the integer representation saves both space and time and you can calculate some things a bit easier with that representation than with the dotted-decimal form. If you are writing or using a tool that perceives an IP address only as a character string or as a set of character strings, you are potentially wasting space by trading a 4-byte, 32-bit representation for a 15-byte, 120-bit representation (worst case).

Furthermore, you are also choosing to use less efficient string comparison code versus integer arithmetic and comparison plus bitwise operations to accomplish the same tasks. Although this may have little to no impact in some scenarios, the repercussions grow significant when you’re dealing with large volumes of IP addresses (and become worse in the IPv6 world) and repeated operations.
\end{quote}
Source: \emph{Data-Driven Security: Analysis, Visualization and Dashboards} Jay Jacobs, Bob Rudis  ISBN: 978-1-118-79372-5


{\bf Hints:}\\
When working with IP addresses always use libraries

{\bf Solution:}\\
When you have pinged the IPv4 address of 2130706433 you are done.
\verb+ping 2130706433+


{\bf Discussion:}\\



\chapter{\faExclamationTriangle\ Data types: IP reputation -- 15min}
\label{siem:ip-reputation}

{\bf Objective:}\\
Find out what IP reputation lists are with some examples.


{\bf Purpose:}\\
Identifying bad things can be hard.

We have a concept named, Indicators of Compromise (IoC).

Indicators of Compromise (IOC) any piece of information that can be used to objectively describe a network intrusion, expressed in a platform-independent manner. Say, if a server connects to THIS specific IP, which is KNOWN to be the control and command server for a malware strain, we conclude the device is infected.

For this we can often download files for identification purposes with some reputation.

{\bf Suggested method:}\\
We will start with the example of
AlienVault’s IP Reputation database

Note: links change, and the link in the book does not work!

\link{http://reputation.alienvault.com/reputation.data}

\link{https://rdrr.io/github/hrbrmstr/netintel/man/Alien.Vault.Reputation.html}


{\bf Hints:}\\
Passive DNS systems exist, which allow you to lookup ol\faExclamationTriangle\ der records, things that have moved.

Maltrail software contains a lot of lists\\
\link{https://github.com/stamparm/maltrail}

{\bf Solution:}\\
When you have understood that data from others can help your identification efforts, you are done.

{\bf Discussion:}\\
We also have used reputation a lot in fighting spam and scam emails.



\chapter{\faInfoCircle\ Research MISP Project 45min}
\label{ex:misp-install}

\hlkimage{9cm}{misp-project-visualization.png}

{\bf Objective:}\\
Research the MISP Project, if you like run it locally on your workstation

Evaluate if this is something you would like to have permanently or during an incident.

{\bf Purpose:}\\
Running MISP Project is  will allow you to fetch reputation lists easily and analyse logs better

{\bf Suggested method:}\\
Go to the web site and look at installation path:\\
\url{https://www.misp-project.org/download/}

You can try to run the application, or we can see the demo on my VM. If we decide to install it, it will take longer.

It may be possible to use a VM image from \url{https://vm.misp-project.org/}

Credentials are:
\begin{alltt}\footnotesize
For the MISP web interface -> admin@admin.test:admin
For the system -> misp:Password1234
\end{alltt}


{\bf Hints:}\\
A VM image is probably fastest, and there may also be Docker images available YMMV.

{\bf Solution:}\\
When you have seen the installation instructions and considered installing it you are done. If you can manage to get it running with the allotted time, great!

{\bf Discussion:}\\
Downloading VM images can be fine for testing, but can be harder to run later. May not be based on the operating system your organisation prefer, can monitor etc.


\chapter{\faExclamationTriangle\ Zeek on the web 10min}
\label{ex:zeekweb}


{\bf Objective:} \\
Try Zeek Network Security Monitor - without installing it.


{\bf Purpose:}\\
Show a couple of examples of Zeek scripting, the built-in language found in Zeek Network Security Monitor


{\bf Suggested method:}\\
Go to \url{http://try.zeek.org/#/?example=hello} and try a few of the examples.

{\bf Hints:}\\
The exercise
\emph{The Summary Statistics Framework} can be run with a specifc PCAP.

\verb+192.168.1.201 did 402 total and 2 unique DNS requests in the last 6 hours.+

{\bf Solution:}\\
You should read the example \emph{Raising a Notice}. Getting output for certain events may be interesting to you.


{\bf Discussion:}\\
Zeek Network Security Monitor is an old/mature tool, but can still be hard to get started using. I would suggest that you always start out using the packages available in your Ubuntu/Debian package repositories.  They work, and will give a first impression of Zeek. If you later want specific features not configured into the binary packages, then install from source.

The tool was renamed in 2018 from Bro to Zeek. Some commands and files still reference the old names. I have opted to use the Debian packages built by the project for our course -- from the openSUSE build service (OBS) repository.

Read more about installing Zeek from: \link{https://docs.zeek.org/en/master/install.html}

Also Zeek uses a zeekctl program to start/stop the tool, and a few config files which we should look at. From a Debian system they can be found in \verb+/opt/zeek/etc/+ :

\begin{alltt}
root@NMS-VM:/opt/zeek/etc/# ls -la
drwxr-xr-x   3 root root  4096 Oct  8 08:36 .
drwxr-xr-x 138 root root 12288 Oct  8 08:36 ..
-rw-r--r--   1 root root  2606 Oct 30  2019 zeekctl.cfg
-rw-r--r--   1 root root   225 Oct 30  2019 networks.cfg
-rw-r--r--   1 root root   644 Oct 30  2019 node.cfg
drwxr-xr-x   2 root root  4096 Oct  8 08:35 site
\end{alltt}


\chapter{\faExclamationTriangle\ Zeek DNS capturing domain names -- 15min}
\label{ex:zeekdnsbasic}


{\bf Objective:} \\
We will now start using Zeek on our systems.


{\bf Purpose:}\\
Try Zeek with example traffic, and see what happens.


{\bf Suggested method packet capture file:}\\
Use Nitroba.pcap can be found in various places around the internet

\begin{minted}[fontsize=\footnotesize]{shell}
$ cd
$ wget http://downloads.digitalcorpora.org/corpora/scenarios/2008-nitroba/nitroba.pcap
$ mkdir $HOME/zeek;cd $HOME/zeek; zeek -r ../nitroba.pcap
... zeek reads the packets
~/zeek$ ls
conn.log  dns.log  dpd.log  files.log  http.log  packet_filter.log
sip.log  ssl.log  weird.log  x509.log
$ less *
\end{minted}

Use :n to jump to the next file in less, go through all of them.

{\bf Suggested method Live traffic:}\\
Make sure Zeek is configured as a standalone probe and configured for the right interface. Linux used to use eth0 as the first ethernet interface, but now can use others, like ens192 or enx00249b1b2991.

\begin{minted}[fontsize=\footnotesize]{shell}
root@debian:/opt/zeek/etc# cat node.cfg
# Example ZeekControl node configuration.
#
# This example has a standalone node ready to go except for possibly changing
# the sniffing interface.

# This is a complete standalone configuration.  Most likely you will
# only need to change the interface.
[zeek]
type=standalone
host=localhost
interface=eth0
...
\end{minted}


{\bf Hints:}\\
There are multiple commands for showing the interfaces and IP addresses on Linux. The old way is using \verb+ifconfig -a+ newer systems would use \verb+ip a+

Note: if your system has a dedicated interface for capturing, you need to turn it on, make it available. This can be done manually using \verb+ifconfig eth0 up+
{\bf Solution:}\\
When you either run Zeek using a packet capture or using live traffic

Running with a capture can be done using a command line such as:
\verb+zeek -r traffic.pcap+

Using zeekctl to start it would be like this:
\begin{minted}[fontsize=\footnotesize]{shell}
// Use the deploy command to initialize and start zeek first
debian:~ root# zeekctl

Welcome to ZeekControl 1.5
Type "help" for help.

[ZeekControl] > install
creating policy directories ...
installing site policies ...
generating standalone-layout.zeek ...
generating local-networks.zeek ...
generating zeekctl-config.zeek ...
generating zeekctl-config.sh ...
...
debian:etc root# grep eth0 node.cfg
interface=eth0
\end{minted}

Afterwards you can stop and start as you wish:
\begin{minted}[fontsize=\footnotesize]{shell}
[ZeekControl] > start
... starting zeek
// Exit using ctrl-d and then look at logs
debian:zeek root# cd /opt/zeek/logs/current
debian:zeek root# pwd
/opt/zeek/logs/current
debian:current root# tail -f dns.log
\end{minted}

You should be able to spot entries like this:
\begin{minted}[fontsize=\footnotesize]{shell}
#fields ts      uid     id.orig_h       id.orig_p       id.resp_h       id.resp_p       proto
trans_id        rtt query   qclass  qclass_name     qtype   qtype_name      rcode   rcode_name
AA      TC      RD      RA      Z       answers TTLs    rejected
1538982372.416180 CD12Dc1SpQm42QW4G3 10.xxx.0.145 57476 10.x.y.141 53	udp
20383	0.045021	www.dr.dk	1	C_INTERNET	1 A 0 NOERROR
F F T T 0 www.dr.dk-v1.edgekey.net,e16198.b.akamaiedge.net,2.17.212.93 60.000000,20409.000000,20.000000 F
\end{minted}

Note: this show ALL the fields captured and dissected by Zeek, there is a nice utility program zeek-cut which can select specific fields:

\begin{minted}[fontsize=\footnotesize]{shell}
root@debian:/opt/zeek/logs/current# cat dns.log | zeek-cut -d ts query answers | grep dr.dk
2018-10-08T09:06:12+0200 www.dr.dk www.dr.dk-v1.edgekey.net,e16198.b.akamaiedge.net,2.17.212.93
\end{minted}

If your file is already in JSON format, you cannot use zeek-cut, but you can use other tools like jQuery \verb+jq+.

{\bf Discussion:}\\
Why is DNS interesting?


\chapter{\faExclamationTriangle\ Zeek TLS capturing certificates -- 15min}
\label{ex:zeektlsbasic}


{\bf Objective:} \\
Run more traffic through Zeek, see the various files.


{\bf Purpose:}\\
See that even though HTTPS and TLS traffic is encrypted it often show names and other values from the certificates and servers.


{\bf Suggested method:}\\
Run Zeek capturing live traffic, start https towards some sites. A lot of common sites today has shifted to HTTPS/TLS.


{\bf Hints:}\\
use zeekctl start and watch the output directory

\begin{alltt}\small
root@debian:/opt/zeek/logs/current# ls *.log
communication.log  dhcp.log files.log known_services.log packet_filter.log  stats.log
stdout.log x509.log conn.log dns.log known_hosts.log loaded_scripts.log  ssl.log
stderr.log weird.log
\end{alltt}

We already looked at \verb+dns.log+, now check \verb+ssl.log+ and \verb+x509.log+

{\bf Solution:}\\
When you have multiple log files with data from Zeek, and have looked into some of them. You are welcome to ask questions and look into more files.


{\bf Discussion:}\\
How can you hide that you are going to HTTPS sites?

Hint: VPN




\chapter{\faExclamationTriangle\ Find Indices in Elasticsearch -- 15min}
\label{ex:es-find-indices}

{\bf Objective:}\\
Find the indices in your Elasticsearch

Note: you might not have any.

{\bf Purpose:}\\
We need to locate our data. Data in Elasticsearch is saved in {\bf indices}.


{\bf Suggested method:}\\
Make sure Elasticsearch is running, with Kibana.

If you have Elasticsearch installed and running try: \link{http://127.0.0.1:9200}

\begin{minted}[fontsize=\footnotesize]{shell}
$ curl 127.0.0.1:9200
\end{minted}
should output:
\begin{minted}[fontsize=\footnotesize]{json}
{
  "name" : "debian-siem",
  "cluster_name" : "elasticsearch",
  "cluster_uuid" : "Kyi6e2WuSGq2TcFzOaPnsQ",
  "version" : {
    "number" : "7.10.0",
    "build_flavor" : "default",
    "build_type" : "deb",
    "build_hash" : "51e9d6f22758d0374a0f3f5c6e8f3a7997850f96",
    "build_date" : "2020-11-09T21:30:33.964949Z",
    "build_snapshot" : false,
    "lucene_version" : "8.7.0",
    "minimum_wire_compatibility_version" : "6.8.0",
    "minimum_index_compatibility_version" : "6.0.0-beta1"
  },
  "tagline" : "You Know, for Search"
}
\end{minted}


If you have Elasticsearch installed and running try: \link{http://127.0.0.1:5601}

Should show a basic Kibana window.
\hlkimage{7cm}{kibana-es-default.png}


If this is your first time in kibana, then you will probably see it asking for an index pattern:
\hlkimage{7cm}{kibana-default-index-pattern.png}

This is a short two-step process, and I choose \verb+filebeat-*+ as I wanted to play with filebeat data, and already had some available.

\hlkimage{10cm}{kibana-filebeat-pattern-1.png}

Make sure to select a time field:

\hlkimage{10cm}{kibana-filebeat-pattern-2.png}

When you have done your first index pattern, note the menu on the left, hamburger icon menu.

From there you can choose: \verb+Stack Management > Index Management+.

\hlkimage{10cm}{kibana-index-mgmt.png}

Feel free to delete data, or otherwise change data in your own installation.



{\bf Hints:}\\
Documentation is available at:\\
\link{https://www.elastic.co/guide/en/elasticsearch/reference/7.10/index-mgmt.html}

{\bf Solution:}\\
When you have ES and Kibana running, and can access the interfaces, you are done.

Before you leave, make note of the menu: Kibana Index Patterns. These are used for presenting data in Kibana, and you want them to show your data!

\begin{quote}
Kibana requires an index pattern to access the Elasticsearch data that you want to explore. An index pattern selects the data to use and allows you to define properties of the fields.
\end{quote}
Source: \link{https://www.elastic.co/guide/en/kibana/current/index-patterns.html}

So if you are running filebeat, and data from filebeat, then it is advised to use an index pattern of \verb+filebeat-*+

\hlkimage{10cm}{kibana-index-pattern.png}

You can create more of these, and common ones would be:
\begin{itemize}
\item \verb+filebeat-*+ -- all sorts of data ingested by filebeat
\item \verb+logstash-*+ -- all sorts of data ingested by logstash

\end{itemize}

{\bf Discussion:}\\
Elasticsearch is a huge system, watching tutorials and playing with the tools are the recommended way to learn.



\chapter{\faExclamationTriangle\ Zeek Data in Elasticsearch -- 30min}
\label{ex:zeek-json-es}

{\bf Objective:}\\
Get data from Zeek files into Elasticsearch.


{\bf Purpose:}\\
Having the files with data is great, but to make it more accessible we will load it into ES.


{\bf Suggested method:}\\
Git pull the kramse-labs repository and do this automatically.

You can also do the steps manually, following a guide like:\\
\link{https://www.elastic.co/blog/collecting-and-analyzing-zeek-data-with-elastic-security}

\link{https://github.com/kramse/kramse-labs}

If you haven't clone, do this first in your Linux VM:
\begin{minted}[fontsize=\footnotesize]{shell}
apt -y install ansible git python
git clone https://github.com/kramse/kramse-labs
\end{minted}

Output should be similar to this:
\begin{minted}[fontsize=\footnotesize]{shell}
user@Projects:t$ git clone https://github.com/kramse/kramse-labs
Cloning into 'kramse-labs'...
remote: Enumerating objects: 283, done.
remote: Total 283 (delta 0), reused 0 (delta 0), pack-reused 283
Receiving objects: 100% (283/283), 215.04 KiB | 906.00 KiB/s, done.
Resolving deltas: 100% (145/145), done.
user@Projects:t$
\end{minted}

If you already cloned, get the latest updates:
\begin{minted}[fontsize=\footnotesize]{shell}
user@Projects:kramse-labs$ git pull
Already up to date.
user@Projects:kramse-labs$
\end{minted}

Run the playbooks for setting up the system {\bf as root}:
\begin{minted}[fontsize=\footnotesize]{shell}
cd kramse-labs/suricatazeek
sudo ansible-playbook -v 1-dependencies.yml 2-suricatazeek.yml 3-elasticstack.yml
\end{minted}

After this, you should have some data in ES/Kibana

Use the discover data menu:
\hlkimage{3cm}{kibana-discover.png}


{\bf Hints:}\\
Make sure you use the right index pattern. Within discover you can change between them with the menu:

\hlkimage{3cm}{kibana-change-index-pattern.png}

Another hint, for lab systems that are turned off and on, use the time selector at the top and select "Last 90 days". This way you can see older data you imported last time. Production systems continually produce data, but your lab server probably has very little.

\hlkimage{7cm}{kibana-90-day-lab.png}

Filebeat comes with predefined assets for parsing, indexing, and visualizing your data. To load these assets run the filebeat setup command:

\begin{minted}[fontsize=\footnotesize]{shell}
filebeat setup -e
\end{minted}

Output should be similar to this:
\begin{minted}[fontsize=\footnotesize]{shell}
Overwriting ILM policy is disabled. Set `setup.ilm.overwrite: true` for enabling.

Index setup finished.
Loading dashboards (Kibana must be running and reachable)
Loaded dashboards
Setting up ML using setup --machine-learning is going to be removed in 8.0.0. Please use the ML app instead.
See more: https://www.elastic.co/guide/en/machine-learning/current/index.html
Loaded machine learning job configurations
Loaded Ingest pipelines
\end{minted}


{\bf Solution:}\\
When you have tried the filebeat tool and seen some data you are done.

{\bf Discussion:}\\



\chapter{\faExclamationTriangle\ Working with dashboards -- 15-60min}
\label{ex:kibana-kts}

{\bf Objective:}\\
Get a few dashboard up and running quickly.

Try the dashboard package KTS7 locally your workstation.


{\bf Purpose:}\\
There are a lot of Dashboards available, such as:\\
\link{https://github.com/StamusNetworks/KTS7}

When learning to create something, a program, document or dashboard -- seeing examples help.

Note: these instructions are very dependent on the version of Elasticsearch and Kibana, so check you version first!
\begin{minted}[fontsize=\footnotesize]{shell}
user@debian-siem:~$ dpkg -l | egrep "elasticsearch|kibana"
ii  elasticsearch    7.10.0       amd64    Distributed RESTful search engine built for the cloud
ii  kibana           7.10.0       amd64    Explore and visualize your Elasticsearch data
\end{minted}

Good news are that the developers of the KTS package has done them for various versions over the years. So look for KTS6 if you are running ES 6.

{\bf Suggested method:}\\
Read instructions for installing the dashboards.

\link{https://github.com/StamusNetworks/KTS7}

\begin{quote}
Kibana 7 Templates for Suricata
Templates/Dashboards for Kibana 7 to use with Suricata. Suricata IDPS/NSM threat hunting and the ELK 7 stack

This repository provides 28 dashboards for the Kibana 7.x and Elasticsearch 7.x for use with Suricata IDS/IPS/NSM - Intrusion Detection, Intrusion Prevention and Network Security Monitoring system

These dashboards are for use with Suricata 6+ and enabled Rust build, Elasticsearch, Logstash, Kibana 7 and comprise of more than 400 visualizations and 24 predefined searches.
\end{quote}

The commands are similar to, first clone:
\begin{minted}[fontsize=\footnotesize]{shell}
git clone https://github.com/StamusNetworks/KTS7.git
cd KTS7
\end{minted}

\eject
then load the dashboards -- JSON files:
\begin{minted}[fontsize=\scriptsize]{shell}
cd API-KIBANA7
curl -X POST "localhost:5601/api/saved_objects/_import" -H 'kbn-xsrf: true' --form file=@index-pattern.ndjson
curl -X POST "localhost:5601/api/saved_objects/_import" -H 'kbn-xsrf: true' --form file=@search.ndjson
curl -X POST "localhost:5601/api/saved_objects/_import" -H 'kbn-xsrf: true' --form file=@visualization.ndjson
curl -X POST "localhost:5601/api/saved_objects/_import" -H 'kbn-xsrf: true' --form file=@dashboard.ndjson
curl -X POST "localhost:5601/api/saved_objects/_import" -H 'kbn-xsrf: true' --form file=@query.ndjson
\end{minted}



{\bf Hints:}\\
Kibana and ES are controlled with web requests, so if you look into the commands, they do HTTP POST requests with JSON files. This allow development of dashboards on one system, and then deploying them easily on others.

{\bf Solution:}\\
When you have tried the tool and seen some dashboards you are done.



{\bf Discussion:}\\
Note: we might not have any data.



\chapter{\faInfoCircle\ Alerting in Eleastic Stack -- 30min}
\label{ex:es7-alerting}

\hlkimage{10cm}{alerting-overview.png}

{\bf Objective:}\\
Go through the Elasticsearch alerting article

{\bf Purpose:}\\
Alerting in ES 7 will allow you to automatically be alerted and initiate actions depending on your data


{\bf Suggested method:}\\
Look at the guide
\link{https://www.elastic.co/guide/en/kibana/7.x/alerting-getting-started.html}



{\bf Hints:}\\
A sending host must be alloweded to connect to some SMTP endpoint, sending email.

Might need a real address to send from, may need some changes to email infrastructure to NOT become tagged as spam!



{\bf Solution:}\\
When you have an idea of the possibilities you are done. Bonus if you actually try making an alert.

{\bf Discussion:}\\
Note the alerting runs on Kibana not Elasticsearch!




\end{document}





\chapter{Wireshark -- 15min}
\label{ex:wireshark-install}

\hlkimage{10cm}{wireshark-http.png}


{\bf Objective:}\\
Try the program Wireshark locally your workstation, or tcpdump

You can run Wireshark on your host too, if you want.

{\bf Purpose:}\\
Installing Wireshark will allow you to analyse packets and protocols

See real network traffic, also know that a lot of information is available and not encrypted.

Note the three way handshake between hosts running TCP. You can either use a browser or command line tools like cURL while capturing

\begin{alltt}
curl http://www.zencurity.com
\end{alltt}


{\bf Suggested method:}\\
Run Wireshark from your Linux

Open Wireshark and start a capture\\
Then in another window execute the ping program while sniffing

or perform a Telnet connection while capturing data

{\bf Hints:}\\
PCAP is a packet capture library allowing you to read packets from the network.
Tcpdump uses libpcap library to read packet from the network cards and save them.
Wireshark is a graphical application to allow you to browse through traffic, packets and protocols.

If not already on your Linux, or do: \verb+apt-get install wireshark+

When running on Linux the network cards are usually named eth0 for the first Ethernet and wlan0 for the first Wireless network card. In Windows the names of the network cards are long and if you cannot see which cards to use then try them one by one.

{\bf Solution:}\\
When you have collected some HTTP/TCP sessions you are done.

If you want to capture packets as a non-root user on Debian, then use the command to add a Wireshark group:
\begin{alltt}
sudo dpkg-reconfigure wireshark-common
\end{alltt}

and add your user to this:
\begin{alltt}
sudo gpasswd -a $USER wireshark
\end{alltt}
Dont forget to logout/login to pick up this new group.

{\bf Discussion:}\\
Wireshark is just an example other packet analyzers exist, some commercial and some open source like Wireshark

We can download a lot of packet traces from around the internet, we might use examples from\\
\link{https://old.zeek.org/community/traces.html}




\chapter{Sniff Your Browser -- 15min}
\label{ex:sniff-captive-portal}


{\bf Objective:}\\
See an example of a simple network application behaviour.

{\bf Purpose:}\\
Learn how to get started analysing network application traffic.

{\bf Suggested method:}\\
Modern browser check if they are online by making requests.

Which requests does a browser make by itself, even though you haven't entered URL yet?

Use Wireshark on your Linux or normal operating system. Start your capture, start your browser.

See if you can identify the traffic.

{\bf Hints:}\\
You should be looking for DNS and HTTP/HTTPS requests.

DNS uses port 53/udp and 53/tcp.

Also googling captive portal and Firefox reveals a setting you can turn of or on.

You might also have observed this when you proxied your browser through Burp suite in an earlier exercise.

{\bf Solution:}\\
When you have identified the traffic belonging to at least one browser you are done. Firefox should be easy.

{\bf Discussion:}\\
Does initiating this from a browser have privacy implications?

Your internet provider can see when you are home, when you start your browser etc. Requests made are often with a lot of extra information, like User-Agent and distinguishable.

Example, my son uses an iPhone, but I use an Android. One user might use Windows 7, while another uses Windows 10 - traffic will be different.





\chapter{ xx -- min}
\label{ex:}

{\bf Objective:}\\
Try the program XX locally your workstation


{\bf Purpose:}\\
Running XXX will allow you to analyse


\begin{alltt}


\end{alltt}


{\bf Suggested method:}\\
Run the program from your Linux VM


{\bf Hints:}\\

{\bf Solution:}\\
When you have tried the tool and seen some data you are done.

{\bf Discussion:}\\




\end{document}
